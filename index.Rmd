---
title: "Final project -   Advanced Topics in Data Mining and Knowledge Discovery"
author: "Micha Amsalem and Ohad Hazan"
date: "23 7 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction 

This is an R Markdown based document for presenting the project results. The main target of the project is to build a predictive model. 

### Loading R packages 
```{r echo=T, results='hide'}
library(tidyverse)
library(descriptr)
library(knitr)
library(ggplot2)
library(mice)
library(lattice)
library(reshape2)
library(DataExplorer)
```

### Uploading the data

```{r}
ffp <- read.csv("ffp_train.csv")
```


### Overview od the first data set - ffp 

Look for NA and missing values:

```{r}
ds_screener(ffp)
```

Summary statistics: 

```{r}
summary(ffp)
```
Database dimensions:  

```{r}
dim(ffp)
```

### convert variables to factors

```{r}
ffp$CALL_FLAG <- as.factor(ffp$CALL_FLAG)
ffp$CREDIT_PROBLEM <- as.factor(ffp$CREDIT_PROBLEM)
ffp$GROUP <- as.factor(ffp$GROUP)
ffp$CHEF_CLUB <- as.factor(ffp$CHEF_CLUB)
ffp$NUM_DEAL <- as.factor(ffp$NUM_DEAL)
ffp$CALL_FLAG <- as.factor(ffp$CALL_FLAG)
ffp$CREDIT_PROBLEM <- as.factor(ffp$CREDIT_PROBLEM)
ffp$RETURN_FLAG <- as.factor(ffp$RETURN_FLAG)
ffp$BENEFIT_FLAG <- as.factor(ffp$BENEFIT_FLAG)
ffp$BUYER_FLAG <- as.factor(ffp$BUYER_FLAG)
```

### Frequency table for last deal price 

```{r}
ds_freq_table(ffp, LAST_DEAL, 4) # 94.4% of costumers last deal price was under 123$ 
```
### Frequency of average days between purchases 

```{r}
ds_freq_table(ffp, ADVANCE_PURCHASE, 4) # 71% between 15-24 days between purchases 
mean(ffp$ADVANCE_PURCHASE) # 20.6 days in average 
```


### TARGET variable - Has the customer purchased following the gift

```{r}
table(ffp$BUYER_FLAG) # Only 4211 did buy the full product 
```
### The company spent 48$*45000 = 2,160,000 and earned 4211*297$ = 1,250,667 - losS of 910K $

# Creating a baseline model using logistic regression

### Convert all variables to numeric 
```{r}
ffp[, 1:21] <- sapply(ffp[, 1:21], as.character)
ffp[, 1:21] <- sapply(ffp[, 1:21], as.numeric)
```



# Correlation heat map
```{r}
plot_correlation(ffp, maxcat = 5L) # Consider removing Group, NUM_DEAL, CREDIT_PROBLEM, RETURN_FLAG - week correlation with BUYER FLAG
```

## Split the data 
#### create a list of random number ranging from 1 to number of rows from actual data and 70% of the data into training data  
```{r}
ffp_index <- sort(sample(nrow(ffp), nrow(ffp)*.7)) 
```


### Creating training data set
```{r}
train <- ffp[ffp_index,]
```

### Creating validation data set 
```{r}
test <- ffp[-ffp_index,]
```

### fit a logistic regression model with the training data set
```{r}
log.model <- glm(BUYER_FLAG ~., data = train, family = binomial(link = "logit"))
```

### Summary table
```{r}
summary(log.model) # CHEF_CLUB, ADVANCE_PURCHACE, CALL_FLAG and BENEFIT_FLAG are significant variables 
```



### Predict - logistic regression model
```{r}
log.predictions <- predict(log.model, test, type="response")
```


### Probabilities top 10
```{r}
head(log.predictions, 10)
```

### Assign labels with decision rule that if the prediction is greater than 0.5, assign it 1 else 0
```{r}
log.prediction.rd <- ifelse(log.predictions > 0.2, 1, 0) # Different cutoffs gives different accuracy measure 
```

### Evaluation of the model using confusion matrix 
```{r}
table(log.prediction.rd, test[,21])
```

# Prediction Accuracy 
```{r}
accuracy <- table(log.prediction.rd, test[,21])
sum(diag(accuracy))/sum(accuracy) # Accuracy = 88%
```

## Logistic regression results 
If we use the log.prediction with 10% cutoff, we would get: 297*383 - (1908+383)*48 = 113,751-109,968 = 3783$ 
If we use the log.prediction with 20% cutoff, we would get: 198*297 - (491+198)*48 = 58,806-33,072 = 25,734$
More then 20% cutoff will not produce better results 


## Model number 2 - Select significant variables only 

```{r}
train__new <- select(train, -one_of('GROUP', "NUM_DEAL", "CREDIT_PROBLEM", "RETURN_FLAG"))
test__new <- select(test, -one_of('GROUP', "NUM_DEAL", "CREDIT_PROBLEM", "RETURN_FLAG"))
```


### fit a logistic regression model with the training data set
```{r}
log.model <- glm(BUYER_FLAG ~., data = train__new, family = binomial(link = "logit"))
summary(log.model) # CHEF_CLUB, ADVANCE_PURCHACE, CALL_FLAG and BENEFIT_FLAG are significant variables 
```

### Predict using logistic new regression model
```{r}
log.predictions <- predict(log.model, test__new, type="response")
```

# Probabilities top 10
```{r}
head(log.predictions, 10)
```

### Assign labels with decision rule that if the prediction is greater than 0.5, assign it 1 else 0
```{r}
log.prediction.rd <- ifelse(log.predictions > 0.2, 1, 0) # Different cutoffs gives different accuracy measure 
```

### Evaluation of the model using confusion matrix 
```{r}
table(log.prediction.rd, test__new[,17])
```
### Accuracy 
```{r}
accuracy <- table(log.prediction.rd, test__new[,17])
sum(diag(accuracy))/sum(accuracy) # Accuracy = 88%
```
### Using less variables didn't change log model accuracy or profits of the company

## Random Forest 
```{r echo=T, results='hide'}
install.packages("randomForest")
library(randomForest)
```


# Convert numeric to factors
```{r}
ffp$GROUP <- as.factor(ffp$GROUP)
ffp$CHEF_CLUB <- as.factor(ffp$CHEF_CLUB)
ffp$NUM_DEAL <- as.factor(ffp$NUM_DEAL)
ffp$CALL_FLAG <- as.factor(ffp$CALL_FLAG)
ffp$CREDIT_PROBLEM <- as.factor(ffp$CREDIT_PROBLEM)
ffp$RETURN_FLAG <- as.factor(ffp$RETURN_FLAG)
ffp$BENEFIT_FLAG <- as.factor(ffp$BENEFIT_FLAG)
ffp$BUYER_FLAG <- as.factor(ffp$BUYER_FLAG)
```


Convert response to factor before running random forest
```{r}
ffp$BUYER_FLAG <- as.factor(ffp$BUYER_FLAG)
train$BUYER_FLAG <- as.factor(train$BUYER_FLAG)
train__new$BUYER_FLAG <- as.factor(train__new$BUYER_FLAG)
```

# Run random forest
```{r}
rf_classifier = randomForest(train__new$BUYER_FLAG ~ ., data=train__new, ntree=300, mtry=4, importance=TRUE) 
rf_classifier
```
```{r}
varImpPlot(rf_classifier)
```

Validation set confusion matrix 
```{r}
prediction_for_table <- predict(rf_classifier,newdata = test__new[,-17])
table(observed=test__new[,17],predicted=prediction_for_table)
```

89.5 % correctly classified by the rf model. Business wise it means that by using this model only 23 will actually buy and 1220 will not

### Try improving the random forest model  

500 trees
```{r}
rf_classifier = randomForest(train__new$BUYER_FLAG ~ ., data=train__new, ntree=500, mtry=4, importance=TRUE) 
rf_classifier 
prediction_for_table <- predict(rf_classifier,test__new[,-17])
table(observed=test__new[,17],predicted=prediction_for_table) # Getting the same OOB error rate as for 300 trees 
```


mtry = 6
```{r}
rf_classifier = randomForest(train__new$BUYER_FLAG ~ ., data=train__new, ntree=500, mtry=6, importance=TRUE) 
rf_classifier 
prediction_for_table <- predict(rf_classifier,test__new[,-17])
table(observed=test__new[,17],predicted=prediction_for_table) # Using 6 for mtry increase the OBB error rate 
```

# Try removing highly correlated variables

```{r}
library(corrplot)
corrplot(cor(test__new %>% select_if(is.numeric), 
                       method = "spearman"))

train_new_nocor <- select(train__new, -one_of("FARE_L_Y2", "FARE_L_Y3", "FARE_L_Y4", "FARE_L_Y5", "POINTS_L_Y2", "POINTS_L_Y3", "POINTS_L_Y4", "POINTS_L_Y5"))

rf_classifier = randomForest(train_new_nocor$BUYER_FLAG ~ ., data=train_new_nocor, ntree=500, mtry=4, importance=TRUE)

rf_classifier 

prediction_for_table <- predict(rf_classifier,test__new[,-17])
```

```{r}
table(observed=test__new[,17],predicted=prediction_for_table) # Better accuracy - only 9.4% OBB error rate. business wise still a loss of 42735$ 
```

# Random forest using ROSE for balancing the data

# Data for Developing Predictive Model
```{r}
table(train$BUYER_FLAG)
prop.table(table(train$BUYER_FLAG))
summary(train)
```


Using ROSE for balancing methods 
```{r}
library(ROSE)
over <- ovun.sample(BUYER_FLAG~., data = train, method = "over", N = 57148)$data
under <- ovun.sample(BUYER_FLAG~., data = train, method = "under", N = 5850)$data
both <- ovun.sample(BUYER_FLAG~., data=train, method = "both", p = 0.5, seed = 222, N = 31499)$data
rose <- ROSE(BUYER_FLAG~., data = train, N = 500, seed=111)$data
```
### Predictive model - Random Forest
```{r}
library(randomForest)
rftrain <- randomForest(BUYER_FLAG~., data = train)
rfover <- randomForest(BUYER_FLAG~., data = over)
rfunder <- randomForest(BUYER_FLAG~., data = under)
rfboth <- randomForest(BUYER_FLAG~., data = both)
rfrose <- randomForest(BUYER_FLAG~., data=rose)
```

### Evaluating the model with test data
```{r}
library(caret)
```

```{r}
names <- c(2:4,17:20)
test[,names] <- lapply(test[,names] , factor)
str(test)
```

```{r}
names <- c(2:4,17:20)
train[,names] <- lapply(train[,names] , factor)
str(train)
```

```{r}
levels(test$NUM_DEAL) <- levels(train$NUM_DEAL)
```


### Evaluating the model with test data
```{r}
confusionMatrix(predict(rftrain, test), test$BUYER_FLAG, positive = '1') # profit = -744$   Sensitivity = 0.004
confusionMatrix(predict(rfunder, test), test$BUYER_FLAG, positive = '1') # profit = 97,792$  Sensitivity = 0.57 
confusionMatrix(predict(rfover, test), test$BUYER_FLAG, positive = '1')  # profit = 22,260$  Sensitivity = 0.077
confusionMatrix(predict(rfboth, test), test$BUYER_FLAG, positive = '1')  # profit = 89,889$  Sensitivity = 0.342
confusionMatrix(predict(rfrose, test), test$BUYER_FLAG, positive = '1')  # profit = 53,037 Sensitivity = 0.61
```





