---
title: "Final project -   Advanced Topics in Data Mining and Knowledge Discovery"
author: "Micha Amsalem and Ohad Hazan"
date: "23 7 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction 

This is an R Markdown based document for presenting the project results. The main target of the project is to build a predictive model. 

### Loading R packages 
```{r echo=T, results='hide'}
library(tidyverse)
library(descriptr)
library(knitr)
library(ggplot2)
library(mice)
library(lattice)
library(reshape2)
library(DataExplorer)
```

### Uploading the data

```{r}
ffp <- read.csv("ffp_train.csv")
```


### Overview od the first data set - ffp 

Look for NA and missing values:

```{r}
ds_screener(ffp)
```

Summary statistics: 

```{r}
summary(ffp)
```
Database dimensions:  

```{r}
dim(ffp)
```

### convert variables to factors

```{r}
ffp$CALL_FLAG <- as.factor(ffp$CALL_FLAG)
ffp$CREDIT_PROBLEM <- as.factor(ffp$CREDIT_PROBLEM)
ffp$GROUP <- as.factor(ffp$GROUP)
ffp$CHEF_CLUB <- as.factor(ffp$CHEF_CLUB)
ffp$NUM_DEAL <- as.factor(ffp$NUM_DEAL)
ffp$CALL_FLAG <- as.factor(ffp$CALL_FLAG)
ffp$CREDIT_PROBLEM <- as.factor(ffp$CREDIT_PROBLEM)
ffp$RETURN_FLAG <- as.factor(ffp$RETURN_FLAG)
ffp$BENEFIT_FLAG <- as.factor(ffp$BENEFIT_FLAG)
ffp$BUYER_FLAG <- as.factor(ffp$BUYER_FLAG)
```

### Frequency table for last deal price 

```{r}
ds_freq_table(ffp, LAST_DEAL, 4) # 94.4% of costumers last deal price was under 123$ 
```
### Frequency of average days between purchases 

```{r}
ds_freq_table(ffp, ADVANCE_PURCHASE, 4) # 71% between 15-24 days between purchases 
mean(ffp$ADVANCE_PURCHASE) # 20.6 days in average 
```


### TARGET variable - Has the customer purchased following the gift

```{r}
table(ffp$BUYER_FLAG) # Only 4211 did buy the full product 
```
### The company spent 48$*45000 = 2,160,000 and earned 4211*297$ = 1,250,667 - losS of 910K $

# Creating a baseline model using logistic regression

### Convert all variables to numeric 
```{r}
ffp[, 1:21] <- sapply(ffp[, 1:21], as.character)
ffp[, 1:21] <- sapply(ffp[, 1:21], as.numeric)
```



# Correlation heat map
```{r}
plot_correlation(ffp, maxcat = 5L) # Consider removing Group, NUM_DEAL, CREDIT_PROBLEM, RETURN_FLAG - week correlation with BUYER FLAG
```

## Split the data 
#### create a list of random number ranging from 1 to number of rows from actual data and 70% of the data into training data  
```{r}
ffp_index <- sort(sample(nrow(ffp), nrow(ffp)*.7)) 
```


### Creating training data set
```{r}
train <- ffp[ffp_index,]
```

### Creating validation data set 
```{r}
test <- ffp[-ffp_index,]
```

### fit a logistic regression model with the training data set
```{r}
log.model <- glm(BUYER_FLAG ~., data = train, family = binomial(link = "logit"))
```

### Summary table
```{r}
summary(log.model) # CHEF_CLUB, ADVANCE_PURCHACE, CALL_FLAG and BENEFIT_FLAG are significant variables 
```



### Predict - logistic regression model
```{r}
log.predictions <- predict(log.model, test, type="response")
```


### Probabilities top 10
```{r}
head(log.predictions, 10)
```

### Assign labels with decision rule that if the prediction is greater than 0.5, assign it 1 else 0
```{r}
log.prediction.rd <- ifelse(log.predictions > 0.2, 1, 0) # Different cutoffs gives different accuracy measure 
```

### Evaluation of the model using confusion matrix 
```{r}
table(log.prediction.rd, test[,21])
```

# Prediction Accuracy 
```{r}
accuracy <- table(log.prediction.rd, test[,21])
sum(diag(accuracy))/sum(accuracy) # Accuracy = 88%
```

## Logistic regression results 
If we use the log.prediction with 10% cutoff, we would get: 297*383 - (1908+383)*48 = 113,751-109,968 = 3783$ 
If we use the log.prediction with 20% cutoff, we would get: 198*297 - (491+198)*48 = 58,806-33,072 = 25,734$
More then 20% cutoff will not produce better results 


## Model number 2 - Select significant variables only 

```{r}
train__new <- select(train, -one_of('GROUP', "NUM_DEAL", "CREDIT_PROBLEM", "RETURN_FLAG"))
test__new <- select(test, -one_of('GROUP', "NUM_DEAL", "CREDIT_PROBLEM", "RETURN_FLAG"))
```


### fit a logistic regression model with the training data set
```{r}
log.model <- glm(BUYER_FLAG ~., data = train__new, family = binomial(link = "logit"))
summary(log.model) # CHEF_CLUB, ADVANCE_PURCHACE, CALL_FLAG and BENEFIT_FLAG are significant variables 
```

### Predict using logistic new regression model
```{r}
log.predictions <- predict(log.model, test__new, type="response")
```

# Probabilities top 10
```{r}
head(log.predictions, 10)
```

### Assign labels with decision rule that if the prediction is greater than 0.5, assign it 1 else 0
```{r}
log.prediction.rd <- ifelse(log.predictions > 0.2, 1, 0) # Different cutoffs gives different accuracy measure 
```

### Evaluation of the model using confusion matrix 
```{r}
table(log.prediction.rd, test__new[,17])
```
# Accuracy 
```{r}
accuracy <- table(log.prediction.rd, test__new[,17])
sum(diag(accuracy))/sum(accuracy) # Accuracy = 88%
```
# Using less variables didn't change log model accuracy or profits of the company






