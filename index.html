<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Micha Amsalem and Ohad Hazan" />


<title>Final project - Advanced Topics in Data Mining and Knowledge Discovery</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 60px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h2 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h3 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h4 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h5 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h6 {
  padding-top: 65px;
  margin-top: -65px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Final project</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About Me</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Final project - Advanced Topics in Data Mining and Knowledge Discovery</h1>
<h4 class="author">Micha Amsalem and Ohad Hazan</h4>
<h4 class="date">23 7 2020</h4>

</div>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>This is an R Markdown based document for presenting the project results. The main target of the project is to build a predictive model.</p>
<div id="loading-r-packages" class="section level3">
<h3>Loading R packages</h3>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## -- Attaching packages --------------------------------------- tidyverse 1.3.0 --</code></pre>
<pre><code>## &lt;U+221A&gt; ggplot2 3.3.0     &lt;U+221A&gt; purrr   0.3.3
## &lt;U+221A&gt; tibble  3.0.0     &lt;U+221A&gt; dplyr   0.8.5
## &lt;U+221A&gt; tidyr   1.0.2     &lt;U+221A&gt; stringr 1.4.0
## &lt;U+221A&gt; readr   1.3.1     &lt;U+221A&gt; forcats 0.5.0</code></pre>
<pre><code>## -- Conflicts ------------------------------------------ tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>library(descriptr)
library(knitr)
library(ggplot2)
library(mice)</code></pre>
<pre><code>## 
## Attaching package: &#39;mice&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     cbind, rbind</code></pre>
<pre class="r"><code>library(lattice)
library(reshape2)</code></pre>
<pre><code>## 
## Attaching package: &#39;reshape2&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:tidyr&#39;:
## 
##     smiths</code></pre>
<pre class="r"><code>library(DataExplorer)</code></pre>
</div>
<div id="uploading-the-data" class="section level3">
<h3>Uploading the data</h3>
<pre class="r"><code>ffp &lt;- read.csv(&quot;ffp_train.csv&quot;)</code></pre>
</div>
<div id="overview-od-the-first-data-set---ffp" class="section level3">
<h3>Overview od the first data set - ffp</h3>
<p>Look for NA and missing values:</p>
<pre class="r"><code>ds_screener(ffp)</code></pre>
<pre><code>## -----------------------------------------------------------------------
## |  Column Name   |  Data Type  |  Levels  |  Missing  |  Missing (%)  |
## -----------------------------------------------------------------------
## |       ID       |   integer   |    NA    |     0     |       0       |
## |     GROUP      |   integer   |    NA    |     0     |       0       |
## |   CHEF_CLUB    |   integer   |    NA    |     0     |       0       |
## |    NUM_DEAL    |   integer   |    NA    |     0     |       0       |
## |   LAST_DEAL    |   numeric   |    NA    |     0     |       0       |
## |ADVANCE_PURCHASE|   integer   |    NA    |     0     |       0       |
## |   FARE_L_Y1    |   numeric   |    NA    |     0     |       0       |
## |   FARE_L_Y2    |   numeric   |    NA    |     0     |       0       |
## |   FARE_L_Y3    |   numeric   |    NA    |     0     |       0       |
## |   FARE_L_Y4    |   numeric   |    NA    |     0     |       0       |
## |   FARE_L_Y5    |   numeric   |    NA    |     0     |       0       |
## |  POINTS_L_Y1   |   numeric   |    NA    |     0     |       0       |
## |  POINTS_L_Y2   |   numeric   |    NA    |     0     |       0       |
## |  POINTS_L_Y3   |   numeric   |    NA    |     0     |       0       |
## |  POINTS_L_Y4   |   numeric   |    NA    |     0     |       0       |
## |  POINTS_L_Y5   |   numeric   |    NA    |     0     |       0       |
## |   CALL_FLAG    |   integer   |    NA    |     0     |       0       |
## | CREDIT_PROBLEM |   integer   |    NA    |     0     |       0       |
## |  RETURN_FLAG   |   integer   |    NA    |     0     |       0       |
## |  BENEFIT_FLAG  |   integer   |    NA    |     0     |       0       |
## |   BUYER_FLAG   |   integer   |    NA    |     0     |       0       |
## -----------------------------------------------------------------------
## 
##  Overall Missing Values           0 
##  Percentage of Missing Values     0 %
##  Rows with Missing Values         0 
##  Columns With Missing Values      0</code></pre>
<p>Summary statistics:</p>
<pre class="r"><code>summary(ffp)</code></pre>
<pre><code>##        ID            GROUP         CHEF_CLUB           NUM_DEAL        LAST_DEAL      ADVANCE_PURCHASE   FARE_L_Y1    
##  Min.   :    1   Min.   :1.000   Min.   :0.000000   Min.   : 0.000   Min.   :  0.00   Min.   : 6.00    Min.   :  0.0  
##  1st Qu.:11251   1st Qu.:2.000   1st Qu.:0.000000   1st Qu.: 3.000   1st Qu.: 26.70   1st Qu.:17.00    1st Qu.:115.0  
##  Median :22501   Median :3.000   Median :0.000000   Median : 4.000   Median : 40.00   Median :20.00    Median :140.0  
##  Mean   :22501   Mean   :3.001   Mean   :0.008867   Mean   : 3.996   Mean   : 49.93   Mean   :20.68    Mean   :150.2  
##  3rd Qu.:33750   3rd Qu.:4.000   3rd Qu.:0.000000   3rd Qu.: 5.000   3rd Qu.: 60.00   3rd Qu.:24.00    3rd Qu.:170.8  
##  Max.   :45000   Max.   :5.000   Max.   :1.000000   Max.   :14.000   Max.   :493.30   Max.   :42.00    Max.   :518.3  
##    FARE_L_Y2       FARE_L_Y3       FARE_L_Y4       FARE_L_Y5      POINTS_L_Y1     POINTS_L_Y2     POINTS_L_Y3   
##  Min.   :  0.0   Min.   :  0.0   Min.   :  0.0   Min.   :  0.0   Min.   :  0.0   Min.   :  0.0   Min.   :  0.0  
##  1st Qu.:115.0   1st Qu.:115.0   1st Qu.:115.0   1st Qu.:115.0   1st Qu.:111.5   1st Qu.:111.5   1st Qu.:111.5  
##  Median :140.0   Median :140.0   Median :140.0   Median :140.0   Median :140.7   Median :140.8   Median :140.8  
##  Mean   :150.2   Mean   :150.3   Mean   :150.2   Mean   :150.1   Mean   :149.6   Mean   :149.6   Mean   :149.6  
##  3rd Qu.:170.8   3rd Qu.:170.8   3rd Qu.:170.8   3rd Qu.:170.8   3rd Qu.:176.5   3rd Qu.:176.6   3rd Qu.:176.6  
##  Max.   :524.2   Max.   :495.8   Max.   :494.2   Max.   :515.8   Max.   :462.2   Max.   :476.2   Max.   :467.5  
##   POINTS_L_Y4     POINTS_L_Y5      CALL_FLAG       CREDIT_PROBLEM     RETURN_FLAG        BENEFIT_FLAG      BUYER_FLAG     
##  Min.   :  0.0   Min.   :  0.0   Min.   :0.00000   Min.   :0.00000   Min.   :0.000000   Min.   :0.0000   Min.   :0.00000  
##  1st Qu.:111.7   1st Qu.:111.4   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.000000   1st Qu.:0.0000   1st Qu.:0.00000  
##  Median :140.8   Median :140.7   Median :0.00000   Median :0.00000   Median :0.000000   Median :0.0000   Median :0.00000  
##  Mean   :149.6   Mean   :149.6   Mean   :0.05031   Mean   :0.09751   Mean   :0.007667   Mean   :0.1274   Mean   :0.09358  
##  3rd Qu.:176.8   3rd Qu.:176.7   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.000000   3rd Qu.:0.0000   3rd Qu.:0.00000  
##  Max.   :501.9   Max.   :498.6   Max.   :1.00000   Max.   :1.00000   Max.   :1.000000   Max.   :1.0000   Max.   :1.00000</code></pre>
<p>Database dimensions:</p>
<pre class="r"><code>dim(ffp)</code></pre>
<pre><code>## [1] 45000    21</code></pre>
</div>
<div id="convert-variables-to-factors" class="section level3">
<h3>convert variables to factors</h3>
<pre class="r"><code>ffp$CALL_FLAG &lt;- as.factor(ffp$CALL_FLAG)
ffp$CREDIT_PROBLEM &lt;- as.factor(ffp$CREDIT_PROBLEM)
ffp$GROUP &lt;- as.factor(ffp$GROUP)
ffp$CHEF_CLUB &lt;- as.factor(ffp$CHEF_CLUB)
ffp$NUM_DEAL &lt;- as.factor(ffp$NUM_DEAL)
ffp$CALL_FLAG &lt;- as.factor(ffp$CALL_FLAG)
ffp$CREDIT_PROBLEM &lt;- as.factor(ffp$CREDIT_PROBLEM)
ffp$RETURN_FLAG &lt;- as.factor(ffp$RETURN_FLAG)
ffp$BENEFIT_FLAG &lt;- as.factor(ffp$BENEFIT_FLAG)
ffp$BUYER_FLAG &lt;- as.factor(ffp$BUYER_FLAG)</code></pre>
</div>
<div id="frequency-table-for-last-deal-price" class="section level3">
<h3>Frequency table for last deal price</h3>
<pre class="r"><code>ds_freq_table(ffp, LAST_DEAL, 4) # 94.4% of costumers last deal price was under 123$ </code></pre>
<pre><code>##                               Variable: LAST_DEAL                               
## |-----------------------------------------------------------------------------|
## |       Bins        | Frequency | Cum Frequency |   Percent    | Cum Percent  |
## |-----------------------------------------------------------------------------|
## |    0    -  123.3  |   42514   |     42514     |    94.48     |    94.48     |
## |-----------------------------------------------------------------------------|
## |  123.3  -  246.7  |   2271    |     44785     |     5.05     |    99.52     |
## |-----------------------------------------------------------------------------|
## |  246.7  -   370   |    201    |     44986     |     0.45     |    99.97     |
## |-----------------------------------------------------------------------------|
## |   370   -  493.3  |    14     |     45000     |     0.03     |     100      |
## |-----------------------------------------------------------------------------|
## |       Total       |   45000   |       -       |    100.00    |      -       |
## |-----------------------------------------------------------------------------|</code></pre>
</div>
<div id="frequency-of-average-days-between-purchases" class="section level3">
<h3>Frequency of average days between purchases</h3>
<pre class="r"><code>ds_freq_table(ffp, ADVANCE_PURCHASE, 4) # 71% between 15-24 days between purchases </code></pre>
<pre><code>##                      Variable: ADVANCE_PURCHASE                       
## |-------------------------------------------------------------------|
## |  Bins   | Frequency | Cum Frequency |   Percent    | Cum Percent  |
## |-------------------------------------------------------------------|
## | 6  - 15 |   5888    |     5888      |    13.08     |    13.08     |
## |-------------------------------------------------------------------|
## | 15 - 24 |   32037   |     37925     |    71.19     |    84.28     |
## |-------------------------------------------------------------------|
## | 24 - 33 |   11596   |     49521     |    25.77     |    110.05    |
## |-------------------------------------------------------------------|
## | 33 - 42 |    420    |     49941     |     0.93     |    110.98    |
## |-------------------------------------------------------------------|
## |  Total  |   45000   |       -       |    100.00    |      -       |
## |-------------------------------------------------------------------|</code></pre>
<pre class="r"><code>mean(ffp$ADVANCE_PURCHASE) # 20.6 days in average </code></pre>
<pre><code>## [1] 20.67869</code></pre>
</div>
<div id="target-variable---has-the-customer-purchased-following-the-gift" class="section level3">
<h3>TARGET variable - Has the customer purchased following the gift</h3>
<pre class="r"><code>table(ffp$BUYER_FLAG) # Only 4211 did buy the full product </code></pre>
<pre><code>## 
##     0     1 
## 40789  4211</code></pre>
</div>
<div id="the-company-spent-4845000-2160000-and-earned-4211297-1250667---loss-of-910k" class="section level3">
<h3>The company spent 48<span class="math inline">\(*45000 = 2,160,000 and earned 4211*297\)</span> = 1,250,667 - losS of 910K $</h3>
</div>
</div>
<div id="creating-a-baseline-model-using-logistic-regression" class="section level1">
<h1>Creating a baseline model using logistic regression</h1>
<div id="convert-all-variables-to-numeric" class="section level3">
<h3>Convert all variables to numeric</h3>
<pre class="r"><code>ffp[, 1:21] &lt;- sapply(ffp[, 1:21], as.character)
ffp[, 1:21] &lt;- sapply(ffp[, 1:21], as.numeric)</code></pre>
</div>
</div>
<div id="correlation-heat-map" class="section level1">
<h1>Correlation heat map</h1>
<pre class="r"><code>plot_correlation(ffp, maxcat = 5L) # Consider removing Group, NUM_DEAL, CREDIT_PROBLEM, RETURN_FLAG - week correlation with BUYER FLAG</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<div id="split-the-data" class="section level2">
<h2>Split the data</h2>
<div id="create-a-list-of-random-number-ranging-from-1-to-number-of-rows-from-actual-data-and-70-of-the-data-into-training-data" class="section level4">
<h4>create a list of random number ranging from 1 to number of rows from actual data and 70% of the data into training data</h4>
<pre class="r"><code>ffp_index &lt;- sort(sample(nrow(ffp), nrow(ffp)*.7)) </code></pre>
</div>
<div id="creating-training-data-set" class="section level3">
<h3>Creating training data set</h3>
<pre class="r"><code>train &lt;- ffp[ffp_index,]</code></pre>
</div>
<div id="creating-validation-data-set" class="section level3">
<h3>Creating validation data set</h3>
<pre class="r"><code>test &lt;- ffp[-ffp_index,]</code></pre>
</div>
<div id="fit-a-logistic-regression-model-with-the-training-data-set" class="section level3">
<h3>fit a logistic regression model with the training data set</h3>
<pre class="r"><code>log.model &lt;- glm(BUYER_FLAG ~., data = train, family = binomial(link = &quot;logit&quot;))</code></pre>
</div>
<div id="summary-table" class="section level3">
<h3>Summary table</h3>
<pre class="r"><code>summary(log.model) # CHEF_CLUB, ADVANCE_PURCHACE, CALL_FLAG and BENEFIT_FLAG are significant variables </code></pre>
<pre><code>## 
## Call:
## glm(formula = BUYER_FLAG ~ ., family = binomial(link = &quot;logit&quot;), 
##     data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.0700  -0.4310  -0.4031  -0.3824   2.4751  
## 
## Coefficients:
##                    Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)      -3.229e+00  1.242e-01 -25.991  &lt; 2e-16 ***
## ID                2.524e-07  1.503e-06   0.168 0.866686    
## GROUP            -4.375e-03  1.382e-02  -0.317 0.751553    
## CHEF_CLUB        -1.036e+00  2.557e-01  -4.054 5.04e-05 ***
## NUM_DEAL          2.079e-02  1.313e-02   1.584 0.113217    
## LAST_DEAL         1.103e-03  6.826e-04   1.616 0.106127    
## ADVANCE_PURCHASE  1.525e-02  4.610e-03   3.307 0.000944 ***
## FARE_L_Y1         1.989e-03  1.701e-03   1.169 0.242315    
## FARE_L_Y2         2.435e-04  2.171e-03   0.112 0.910699    
## FARE_L_Y3        -4.410e-03  2.178e-03  -2.025 0.042905 *  
## FARE_L_Y4         5.681e-03  2.178e-03   2.608 0.009095 ** 
## FARE_L_Y5         4.972e-05  1.903e-03   0.026 0.979162    
## POINTS_L_Y1      -2.977e-04  9.073e-04  -0.328 0.742837    
## POINTS_L_Y2       4.334e-04  1.131e-03   0.383 0.701591    
## POINTS_L_Y3      -3.466e-04  1.115e-03  -0.311 0.755974    
## POINTS_L_Y4       2.823e-04  1.122e-03   0.252 0.801363    
## POINTS_L_Y5      -1.086e-03  1.001e-03  -1.085 0.277817    
## CALL_FLAG         1.317e+00  6.254e-02  21.064  &lt; 2e-16 ***
## CREDIT_PROBLEM   -4.784e-02  6.682e-02  -0.716 0.473968    
## RETURN_FLAG      -1.666e-02  2.267e-01  -0.074 0.941398    
## BENEFIT_FLAG      1.690e-01  6.088e-02   2.775 0.005516 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 19750  on 31498  degrees of freedom
## Residual deviance: 19066  on 31478  degrees of freedom
## AIC: 19108
## 
## Number of Fisher Scoring iterations: 5</code></pre>
</div>
<div id="predict---logistic-regression-model" class="section level3">
<h3>Predict - logistic regression model</h3>
<pre class="r"><code>log.predictions &lt;- predict(log.model, test, type=&quot;response&quot;)</code></pre>
</div>
<div id="probabilities-top-10" class="section level3">
<h3>Probabilities top 10</h3>
<pre class="r"><code>head(log.predictions, 10)</code></pre>
<pre><code>##          4          6          7          8         10         11         13         17         19         20 
## 0.07109688 0.08491578 0.07206675 0.07316841 0.10679302 0.06514456 0.14403890 0.07503545 0.08339935 0.12117595</code></pre>
</div>
<div id="assign-labels-with-decision-rule-that-if-the-prediction-is-greater-than-0.5-assign-it-1-else-0" class="section level3">
<h3>Assign labels with decision rule that if the prediction is greater than 0.5, assign it 1 else 0</h3>
<pre class="r"><code>log.prediction.rd &lt;- ifelse(log.predictions &gt; 0.2, 1, 0) # Different cutoffs gives different accuracy measure </code></pre>
</div>
<div id="evaluation-of-the-model-using-confusion-matrix" class="section level3">
<h3>Evaluation of the model using confusion matrix</h3>
<pre class="r"><code>table(log.prediction.rd, test[,21])</code></pre>
<pre><code>##                  
## log.prediction.rd     0     1
##                 0 11762  1034
##                 1   514   191</code></pre>
</div>
</div>
</div>
<div id="prediction-accuracy" class="section level1">
<h1>Prediction Accuracy</h1>
<pre class="r"><code>accuracy &lt;- table(log.prediction.rd, test[,21])
sum(diag(accuracy))/sum(accuracy) # Accuracy = 88%</code></pre>
<pre><code>## [1] 0.8853418</code></pre>
<div id="logistic-regression-results" class="section level2">
<h2>Logistic regression results</h2>
<p>If we use the log.prediction with 10% cutoff, we would get: 297<em>383 - (1908+383)</em>48 = 113,751-109,968 = 3783$ If we use the log.prediction with 20% cutoff, we would get: 198<em>297 - (491+198)</em>48 = 58,806-33,072 = 25,734$ More then 20% cutoff will not produce better results</p>
</div>
<div id="model-number-2---select-significant-variables-only" class="section level2">
<h2>Model number 2 - Select significant variables only</h2>
<pre class="r"><code>train__new &lt;- select(train, -one_of(&#39;GROUP&#39;, &quot;NUM_DEAL&quot;, &quot;CREDIT_PROBLEM&quot;, &quot;RETURN_FLAG&quot;))
test__new &lt;- select(test, -one_of(&#39;GROUP&#39;, &quot;NUM_DEAL&quot;, &quot;CREDIT_PROBLEM&quot;, &quot;RETURN_FLAG&quot;))</code></pre>
<div id="fit-a-logistic-regression-model-with-the-training-data-set-1" class="section level3">
<h3>fit a logistic regression model with the training data set</h3>
<pre class="r"><code>log.model &lt;- glm(BUYER_FLAG ~., data = train__new, family = binomial(link = &quot;logit&quot;))
summary(log.model) # CHEF_CLUB, ADVANCE_PURCHACE, CALL_FLAG and BENEFIT_FLAG are significant variables </code></pre>
<pre><code>## 
## Call:
## glm(formula = BUYER_FLAG ~ ., family = binomial(link = &quot;logit&quot;), 
##     data = train__new)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.0629  -0.4306  -0.4030  -0.3831   2.4815  
## 
## Coefficients:
##                    Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)      -3.164e+00  1.046e-01 -30.253  &lt; 2e-16 ***
## ID                2.722e-07  1.503e-06   0.181 0.856263    
## CHEF_CLUB        -1.037e+00  2.557e-01  -4.056    5e-05 ***
## LAST_DEAL         3.772e-04  5.086e-04   0.742 0.458279    
## ADVANCE_PURCHASE  1.525e-02  4.610e-03   3.307 0.000942 ***
## FARE_L_Y1         2.027e-03  1.700e-03   1.192 0.233158    
## FARE_L_Y2         3.098e-04  2.169e-03   0.143 0.886455    
## FARE_L_Y3        -4.353e-03  2.176e-03  -2.000 0.045467 *  
## FARE_L_Y4         5.711e-03  2.176e-03   2.624 0.008692 ** 
## FARE_L_Y5         1.047e-04  1.903e-03   0.055 0.956123    
## POINTS_L_Y1      -3.113e-04  9.071e-04  -0.343 0.731455    
## POINTS_L_Y2       4.442e-04  1.131e-03   0.393 0.694479    
## POINTS_L_Y3      -3.453e-04  1.115e-03  -0.310 0.756814    
## POINTS_L_Y4       2.737e-04  1.122e-03   0.244 0.807305    
## POINTS_L_Y5      -1.079e-03  1.001e-03  -1.078 0.280991    
## CALL_FLAG         1.318e+00  6.253e-02  21.083  &lt; 2e-16 ***
## BENEFIT_FLAG      1.690e-01  6.088e-02   2.776 0.005500 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 19750  on 31498  degrees of freedom
## Residual deviance: 19069  on 31482  degrees of freedom
## AIC: 19103
## 
## Number of Fisher Scoring iterations: 5</code></pre>
</div>
<div id="predict-using-logistic-new-regression-model" class="section level3">
<h3>Predict using logistic new regression model</h3>
<pre class="r"><code>log.predictions &lt;- predict(log.model, test__new, type=&quot;response&quot;)</code></pre>
</div>
</div>
</div>
<div id="probabilities-top-10-1" class="section level1">
<h1>Probabilities top 10</h1>
<pre class="r"><code>head(log.predictions, 10)</code></pre>
<pre><code>##          4          6          7          8         10         11         13         17         19         20 
## 0.06889731 0.08234937 0.06818434 0.07616351 0.10696622 0.06651044 0.14407917 0.08387526 0.07770839 0.12231423</code></pre>
<div id="assign-labels-with-decision-rule-that-if-the-prediction-is-greater-than-0.5-assign-it-1-else-0-1" class="section level3">
<h3>Assign labels with decision rule that if the prediction is greater than 0.5, assign it 1 else 0</h3>
<pre class="r"><code>log.prediction.rd &lt;- ifelse(log.predictions &gt; 0.2, 1, 0) # Different cutoffs gives different accuracy measure </code></pre>
</div>
<div id="evaluation-of-the-model-using-confusion-matrix-1" class="section level3">
<h3>Evaluation of the model using confusion matrix</h3>
<pre class="r"><code>table(log.prediction.rd, test__new[,17])</code></pre>
<pre><code>##                  
## log.prediction.rd     0     1
##                 0 11762  1034
##                 1   514   191</code></pre>
</div>
<div id="accuracy" class="section level3">
<h3>Accuracy</h3>
<pre class="r"><code>accuracy &lt;- table(log.prediction.rd, test__new[,17])
sum(diag(accuracy))/sum(accuracy) # Accuracy = 88%</code></pre>
<pre><code>## [1] 0.8853418</code></pre>
</div>
<div id="using-less-variables-didnt-change-log-model-accuracy-or-profits-of-the-company" class="section level3">
<h3>Using less variables didn’t change log model accuracy or profits of the company</h3>
</div>
<div id="random-forest" class="section level2">
<h2>Random Forest</h2>
<pre class="r"><code>install.packages(&quot;randomForest&quot;)</code></pre>
<pre><code>## Installing package into &#39;C:/Users/user/Documents/R/win-library/3.6&#39;
## (as &#39;lib&#39; is unspecified)</code></pre>
<pre class="r"><code>library(randomForest)</code></pre>
<pre><code>## randomForest 4.6-14</code></pre>
<pre><code>## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<pre><code>## 
## Attaching package: &#39;randomForest&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     combine</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     margin</code></pre>
</div>
</div>
<div id="convert-numeric-to-factors" class="section level1">
<h1>Convert numeric to factors</h1>
<pre class="r"><code>ffp$GROUP &lt;- as.factor(ffp$GROUP)
ffp$CHEF_CLUB &lt;- as.factor(ffp$CHEF_CLUB)
ffp$NUM_DEAL &lt;- as.factor(ffp$NUM_DEAL)
ffp$CALL_FLAG &lt;- as.factor(ffp$CALL_FLAG)
ffp$CREDIT_PROBLEM &lt;- as.factor(ffp$CREDIT_PROBLEM)
ffp$RETURN_FLAG &lt;- as.factor(ffp$RETURN_FLAG)
ffp$BENEFIT_FLAG &lt;- as.factor(ffp$BENEFIT_FLAG)
ffp$BUYER_FLAG &lt;- as.factor(ffp$BUYER_FLAG)</code></pre>
<p>Convert response to factor before running random forest</p>
<pre class="r"><code>ffp$BUYER_FLAG &lt;- as.factor(ffp$BUYER_FLAG)
train$BUYER_FLAG &lt;- as.factor(train$BUYER_FLAG)
train__new$BUYER_FLAG &lt;- as.factor(train__new$BUYER_FLAG)</code></pre>
</div>
<div id="run-random-forest" class="section level1">
<h1>Run random forest</h1>
<pre class="r"><code>rf_classifier = randomForest(train__new$BUYER_FLAG ~ ., data=train__new, ntree=300, mtry=4, importance=TRUE) 
rf_classifier</code></pre>
<pre><code>## 
## Call:
##  randomForest(formula = train__new$BUYER_FLAG ~ ., data = train__new,      ntree = 300, mtry = 4, importance = TRUE) 
##                Type of random forest: classification
##                      Number of trees: 300
## No. of variables tried at each split: 4
## 
##         OOB estimate of  error rate: 9.66%
## Confusion matrix:
##       0   1 class.error
## 0 28401 112 0.003928033
## 1  2930  56 0.981245814</code></pre>
<pre class="r"><code>varImpPlot(rf_classifier)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>Validation set confusion matrix</p>
<pre class="r"><code>prediction_for_table &lt;- predict(rf_classifier,newdata = test__new[,-17])
table(observed=test__new[,17],predicted=prediction_for_table)</code></pre>
<pre><code>##         predicted
## observed     0     1
##        0 12241    35
##        1  1197    28</code></pre>
<p>89.5 % correctly classified by the rf model. Business wise it means that by using this model only 23 will actually buy and 1220 will not</p>
<div id="try-improving-the-random-forest-model" class="section level3">
<h3>Try improving the random forest model</h3>
<p>500 trees</p>
<pre class="r"><code>rf_classifier = randomForest(train__new$BUYER_FLAG ~ ., data=train__new, ntree=500, mtry=4, importance=TRUE) 
rf_classifier </code></pre>
<pre><code>## 
## Call:
##  randomForest(formula = train__new$BUYER_FLAG ~ ., data = train__new,      ntree = 500, mtry = 4, importance = TRUE) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 4
## 
##         OOB estimate of  error rate: 9.64%
## Confusion matrix:
##       0  1 class.error
## 0 28422 91 0.003191527
## 1  2945 41 0.986269257</code></pre>
<pre class="r"><code>prediction_for_table &lt;- predict(rf_classifier,test__new[,-17])
table(observed=test__new[,17],predicted=prediction_for_table) # Getting the same OOB error rate as for 300 trees </code></pre>
<pre><code>##         predicted
## observed     0     1
##        0 12242    34
##        1  1199    26</code></pre>
<p>mtry = 6</p>
<pre class="r"><code>rf_classifier = randomForest(train__new$BUYER_FLAG ~ ., data=train__new, ntree=500, mtry=6, importance=TRUE) 
rf_classifier </code></pre>
<pre><code>## 
## Call:
##  randomForest(formula = train__new$BUYER_FLAG ~ ., data = train__new,      ntree = 500, mtry = 6, importance = TRUE) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 6
## 
##         OOB estimate of  error rate: 9.76%
## Confusion matrix:
##       0   1 class.error
## 0 28373 140 0.004910041
## 1  2935  51 0.982920295</code></pre>
<pre class="r"><code>prediction_for_table &lt;- predict(rf_classifier,test__new[,-17])
table(observed=test__new[,17],predicted=prediction_for_table) # Using 6 for mtry increase the OBB error rate </code></pre>
<pre><code>##         predicted
## observed     0     1
##        0 12230    46
##        1  1197    28</code></pre>
</div>
</div>
<div id="try-removing-highly-correlated-variables" class="section level1">
<h1>Try removing highly correlated variables</h1>
<p>library(corrplot) corrplot(cor(test__new %&gt;% select_if(is.numeric), method = “spearman”)) train_new_nocor &lt;- select(train__new, -one_of(“FARE_L_Y2”, “FARE_L_Y3”, “FARE_L_Y4”, “FARE_L_Y5”, “POINTS_L_Y2”, “POINTS_L_Y3”, “POINTS_L_Y4”, “POINTS_L_Y5”)) rf_classifier = randomForest(train_new_nocor<span class="math inline">\(BUYER_FLAG ~ ., data=train_new_nocor, ntree=500, mtry=4, importance=TRUE) rf_classifier prediction_for_table &lt;- predict(rf_classifier,test__new[,-17]) table(observed=test__new[,17],predicted=prediction_for_table) # Better accuracy - only 9.4% OBB error rate. business wise still a loss of 42735\)</span></p>
<div id="section" class="section level55">
<p class="heading"></p>
</div>
<div id="random-forest-using-rose-for-balancing-the-data" class="section level3">
<h3>Random forest using ROSE for balancing the data</h3>
<div id="section-1" class="section level55">
<p class="heading"></p>
</div>
</div>
</div>
<div id="data-for-developing-predictive-model" class="section level1">
<h1>Data for Developing Predictive Model</h1>
<p>table(train<span class="math inline">\(BUYER_FLAG) prop.table(table(train\)</span>BUYER_FLAG)) summary(train)</p>
</div>
<div id="using-rose-for-balancing-methods" class="section level1">
<h1>Using ROSE for balancing methods</h1>
<p>library(ROSE) over &lt;- ovun.sample(BUYER_FLAG~., data = train, method = “over”, N = 57148)<span class="math inline">\(data under &lt;- ovun.sample(BUYER_FLAG~., data = train, method = &quot;under&quot;, N = 5850)\)</span>data both &lt;- ovun.sample(BUYER_FLAG~., data=train, method = “both”, p = 0.5, seed = 222, N = 31499)<span class="math inline">\(data rose &lt;- ROSE(BUYER_FLAG~., data = train, N = 500, seed=111)\)</span>data</p>
<div id="predictive-model---random-forest" class="section level3">
<h3>Predictive model - Random Forest</h3>
<p>library(randomForest) rftrain &lt;- randomForest(BUYER_FLAG~., data = train) rfover &lt;- randomForest(BUYER_FLAG~., data = over) rfunder &lt;- randomForest(BUYER_FLAG~., data = under) rfboth &lt;- randomForest(BUYER_FLAG~., data = both) rfrose &lt;- randomForest(BUYER_FLAG~., data=rose)</p>
</div>
<div id="evaluating-the-model-with-test-data" class="section level3">
<h3>Evaluating the model with test data</h3>
<p>library(caret)</p>
<p>names &lt;- c(2:4,17:20) test[,names] &lt;- lapply(test[,names] , factor) str(test)</p>
<p>names &lt;- c(2:4,17:20) train[,names] &lt;- lapply(train[,names] , factor) str(train)</p>
<p>levels(test<span class="math inline">\(NUM_DEAL) &lt;- levels(train\)</span>NUM_DEAL)</p>
</div>
<div id="evaluating-the-model-with-test-data-1" class="section level3">
<h3>Evaluating the model with test data</h3>
<p>confusionMatrix(predict(rftrain, test), test<span class="math inline">\(BUYER_FLAG, positive = &#39;1&#39;) # profit = -744\)</span> Sensitivity = 0.004 confusionMatrix(predict(rfunder, test), test<span class="math inline">\(BUYER_FLAG, positive = &#39;1&#39;) # profit = 97,792\)</span> Sensitivity = 0.57 confusionMatrix(predict(rfover, test), test<span class="math inline">\(BUYER_FLAG, positive = &#39;1&#39;) # profit = 22,260\)</span> Sensitivity = 0.077 confusionMatrix(predict(rfboth, test), test<span class="math inline">\(BUYER_FLAG, positive = &#39;1&#39;) # profit = 89,889\)</span> Sensitivity = 0.342 confusionMatrix(predict(rfrose, test), test$BUYER_FLAG, positive = ‘1’) # profit = 53,037 Sensitivity = 0.61</p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
